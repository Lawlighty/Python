# https://www.mzitu.com/171353  初始链接
import requests
from bs4 import BeautifulSoup
import re
import os
import time

def getHtmlText(url):
    kv = {
        'user-agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3650.400 QQBrowser/10.4.3341.400',
        # 'Host':'i.meizitu.net',
        # 'Referer':'https://www.mzitu.com/tag/youhuo/'
    }
    try:
        r = requests.get(url, headers= kv)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        print('html error')

def getHtml(url):
    kv = {
        'user-agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3650.400 QQBrowser/10.4.3341.400',
        'Host': 'i.meizitu.net',
        'Referer': 'https://www.mzitu.com/tag/youhuo/'
    }
    try:
        r = requests.get(url, headers= kv)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r
    except:
        print('html22 error')

# 获得HTML页面中图片信息
def getMainPciInfo(html):
    try:
        soup = BeautifulSoup(html, 'html.parser')
        pic_info = soup.find('div',attrs={'class':'main-image'})
        #图片下载链接
        pic_url = pic_info.find('a').find('img').get('src')
        print('pic_url:**************',pic_url)
        #保存文件夹名
        div_name = soup.find('h2',attrs={'class':'main-title'}).text.split(' ')[0]
        print('div_name:*************',div_name)

        SavePic(div_name,pic_url)

    except:
        print('get pic error')

#保存图片
def SavePic(div_name, pic_url):
    pic_name = pic_url.split('/')[-1]

    create_file('pic/{}'.format(div_name))
    print('ok')
    r = getHtml(pic_url)
    print('开始保存:',pic_name)
    #二进制形式保存
    with open('pic/{}/{}'.format(div_name,pic_name ), 'wb') as f:
        f.write(r.content)
        print('保存成功')
        time.sleep(1)

#根据图片标题创建对应文件夹
def create_file(name):
    if not os.path.exists(name):
        os.makedirs(name)

#获得套图链接 该链接有44页(默认)
def url_deep(url, deep=44):
    for i in range(1,45):
        nowurl = url+'/'+str(i)
        html = getHtmlText(nowurl)
        getMainPciInfo(html)

if __name__ == '__main__':
    url = 'https://www.mzitu.com/171353'
    # html = getHtmlText(url)
    # getMainPciInfo(html)
    url_deep(url)
    print('end')
