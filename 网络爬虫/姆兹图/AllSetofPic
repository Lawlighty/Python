
import requests
from bs4 import BeautifulSoup
import re
import os
import time

def getHtmlText(url):
    kv = {
        'user-agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3650.400 QQBrowser/10.4.3341.400',
        # 'Host':'i.meizitu.net',
        # 'Referer':'https://www.mzitu.com/tag/youhuo/'
    }
    try:
        r = requests.get(url, headers= kv)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        print('html error')

def getHtml(url):
    kv = {
        'user-agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3650.400 QQBrowser/10.4.3341.400',
        'Host': 'i.meizitu.net',
        'Referer': 'https://www.mzitu.com/tag/youhuo/'
    }
    try:
        r = requests.get(url, headers= kv)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r
    except:
        print('html22 error')

# 获得HTML页面中图片信息
def getMainPciInfo(html, divName):
    try:
        soup = BeautifulSoup(html, 'html.parser')
        pic_info = soup.find('div',attrs={'class':'main-image'})
        #图片下载链接
        pic_url = pic_info.find('a').find('img').get('src')
        print('pic_url:**************',pic_url)
        #保存文件夹名
        div_name = divName
        # div_name = soup.find('h2',attrs={'class':'main-title'}).text.split(' ')[0]
        print('div_name:*************',div_name)

        SavePic(div_name,pic_url)

    except:
        print('get pic error')

#保存图片
def SavePic(div_name, pic_url):
    pic_name = pic_url.split('/')[-1]

    create_file('pic/{}'.format(div_name))
    print('ok')
    r = getHtml(pic_url)
    print('开始保存:',pic_name)
    #二进制形式保存
    with open('pic/{}/{}'.format(div_name,pic_name ), 'wb') as f:
        f.write(r.content)
        print('保存成功')
        time.sleep(1)

#根据图片标题创建对应文件夹
def create_file(name):
    if not os.path.exists(name):
        os.makedirs(name)

#获得套图图片链接 该链接有44页(默认)
def url_deep(url, divName, deep=44 ):
    deep = int(deep)+1
    for i in range(1,deep):
        nowurl = url+'/'+str(i)
        html = getHtmlText(nowurl)
        getMainPciInfo(html, divName)

#获得所有套图链接
def get_links(url):
    html = getHtmlText(url)
    soup = BeautifulSoup(html, 'html.parser')
    link_list = soup.find('div',attrs={'class':'all'}).find('ul', attrs={'class', 'archives'}).find_all('a', attrs={'target': '_blank'})
    for i in link_list:
        url = i.get('href')
        print('now url:',url)
        divName = i.text
        print('now divName:',divName)
        deep = get_deeps(url)
        print('now deepp', deep)
        # deep = int(deep)
        url_deep(url, divName, deep)

#获得当前套图页数
def get_deeps(url):
    html = getHtmlText(url)
    soup = BeautifulSoup(html, 'html.parser')
    deep = soup.find('div', attrs={'class': 'pagenavi'}).find_all('span')[6].text
    return deep


if __name__ == '__main__':
    url = 'https://www.mzitu.com/all/'
    get_links(url)
    print('end')
